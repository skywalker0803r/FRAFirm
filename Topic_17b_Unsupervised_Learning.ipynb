{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdOu7NafOSvzmtpEuPCj34"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Topic 17b: Unsupervised Learning"],"metadata":{"id":"6T6R0nn2NheX"}},{"cell_type":"markdown","source":["This is the code for Topic 17 that wouldn't run on a mac.\n","\n","## K-means algorithm\n","\n","The first issue arose when we tried to run the K-means clustering algorithm.\n","\n","Here is the algorithm for the K-means clustering:\n","\n","1. Initialize the number _K_, which is the number of clusters to find.\n","1. Initialize the centroids (the location of the center) of the _K_ clusters.\n","1. Decide the class memberships of the _N_ samples: for each sample, place it into the cluster that has the closest centroid.\n","1. Revise the centroids of the clusters: each centroid moves to the mean of the locations for all the samples in that cluster.\n","1. If none of the samples changed clusters in the previous step, the algorithm terminates.  Otherwise, go bad to step 3.\n","\n","As usual, we install the packages we will need in this notebook (although we can add more later):\n","\n"],"metadata":{"id":"vfqURX0cNzRg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErmsgjpZGFMn"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","from sklearn import cluster\n","import matplotlib.pylab as plt"]},{"cell_type":"markdown","source":["Now we will create some data.  We will generate three clusters of 40 points each.  We center the points for these clusters around these locations: [0,0], [5,5], and [8,3].  These will be in the X array, and each entry will have two values: the x- and y-coordinate.\n","\n","We will also build an 'answer' array, y.  This array indicates the cluster membership for each sample, with the first 40 samples being in cluster 1, the next 40 in cluster 2, and the remaining in cluster 3.  Normally, we would not have this data (for unsupervised learning we don't have the 'answers').  However, to help us see what is going on, we create this data.  We won't pass these values to the machine learning code, but we will use this to color some of our plots.\n"],"metadata":{"id":"PzQZhBBkWTsa"}},{"cell_type":"code","source":["MAXN = 40\n","X = np.concatenate([1.25*np.random.randn(MAXN,2), 5 + 1.5*np.random.randn(MAXN,2)])\n","X = np.concatenate([X, [8,3] + 1.2*np.random.randn(MAXN, 2)])\n","\n","y = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\n","y = np.concatenate([y,3*np.ones((MAXN,1))])"],"metadata":{"id":"HWrWPQW2GbPi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following two plots, we see the data.  In the left plot, we have colored the points to show which cluster they were created within.  The right plot shows the data that the algorithm sees: Just the points, with no clue as to the coloring."],"metadata":{"id":"DMbwGQUyXtvA"}},{"cell_type":"code","source":["plt.rc('figure', figsize = (12, 5))\n","plt.subplot(1,2,1)\n","plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n","plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n","plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n","plt.title('Data as were generated')\n","\n","plt.subplot(1,2,2)\n","plt.scatter(X[:,0],X[:,1],color='r')\n","plt.title('Data as the algorithm sees them')\n","plt.show()"],"metadata":{"id":"dFZVUYOMIM7Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following we will do the actual clustering.  There are techniques that can help us determine the number of clusters to use, but for now we will simply say that there should be 3 clusters."],"metadata":{"id":"ZTFugYcdXrrf"}},{"cell_type":"code","source":["K = 3\n","clf = cluster.KMeans(init='random', n_clusters=K, n_init=10, max_iter=300, random_state=12)\n","clf.fit(X)"],"metadata":{"id":"j8XQcVs9G0_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* \"init='random'\" tells the algorithm to randomize the initial locations of the centroids..\n","* \"n_clusters\" sets the number of clusters.\n","* \"n_init\" says to run the whole process 10 times.  The _K-means_ clusterizer can get stuck in local minima, so by running multiple times we try to avoid these less-optimal solutions.\n","* \"max_iter\": Recall that _K-means_ iteratively refines the centroids of the clusters.  The process stops when the samples do not change clusters.  However, sometimes the process may iterate with very minimal changes, so this factor can be used to make sure the process terminates.\n","* \"random_state\" initializes the random number generator, as was mentioned in other lectures.\n","\n","The *labels_* attribute of the clustering method indicates the final answer, showing for each sample which cluster it was assigned:"],"metadata":{"id":"2sg2jE7za8l-"}},{"cell_type":"code","source":["print(clf.labels_)"],"metadata":{"id":"KXw6BUR0HKdz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ideally, the first 40 points would all belong to one cluster, the next 40 in a second cluster, and the remaining points in the third cluster.  We see, however, there are a few points that are assigned to the wrong cluster.\n","\n","Let's look again at the plot:"],"metadata":{"id":"brQZ9aoGcmIe"}},{"cell_type":"code","source":["plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n","plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n","plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n","fig = plt.gcf()\n","fig.set_size_inches((6, 5))"],"metadata":{"id":"d5EQpMeiJ6MA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the blue and green clusters overlap, so there are some green points deep within the blue territory, and a blue dot close to the center of the green.  Without the coloring information, how is the clustering program supposed to know the correct placement of these colors?  We cannot expect the program to get 100% accuracy!\n","\n","What we want to do next is to draw the territories for each of the clusters.  How do we do that?  The technique they use is this:\n","\n","* Build a _mesh grid_, an array of 200 x 200 points covering the area of our drawing.  \n","* Take all of those grid points and run them through the trained clustering algorithm.  This will then indicate the cluster that would be assigned to each of the grid points.  \n","* Use _implot_ to plot the mesh \"image\".  This plot will be drawn a little blurred, so the individual points merge.  \n","* Draw the original data points over the top."],"metadata":{"id":"9CSJNnlpgLo7"}},{"cell_type":"code","source":["x = np.linspace(-5,15, 200)\n","XX, YY = np.meshgrid(x, x)\n","sz = XX.shape\n","data = np.c_[XX.ravel(), YY.ravel()]\n","Z = clf.predict(data)"],"metadata":{"id":"gf2PVQ7pKAcn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5, 15, -5, 15), alpha=0.3, vmin=0, vmax=K-1)\n","plt.title('Space partitions', size=14)\n","plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n","plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n","plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n","fig = plt.gcf()\n","fig.set_size_inches((6, 5))\n","plt.show()"],"metadata":{"id":"6miNUWEDLJue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The plot shows that the clusters were separated quite nicely, and we can also see that a few of the points are 'mis-categorized'.  Again, the program actually did the right thing, those 'incorrect' points are really inside the limits of the other cluster."],"metadata":{"id":"BG8-kwOuiu8v"}},{"cell_type":"code","source":["clf = cluster.KMeans(n_clusters=K, random_state=0)\n","clf.fit(X)\n","\n","data=np.c_[XX.ravel(),YY.ravel()]\n","Z=clf.predict(data)"],"metadata":{"id":"Kkgoj0zvLo6D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title('Final result of K-means', size=14)\n","\n","plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n","plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n","plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n","\n","plt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower', extent=(-5, 15, -5, 15), alpha=0.3, vmin=0, vmax=K-1)\n","\n","x = np.linspace(-5,15, 200)\n","XX, YY = np.meshgrid(x, x)\n","\n","fig = plt.gcf()\n","fig.set_size_inches((6, 5))\n","\n","plt.show()"],"metadata":{"id":"Rn34I46jMRAk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The _K-means_ algorithm clusters the data by finding the optimal _centroids_ for the clusters.  It chooses these centroids by minimizing a criterion known as the _inertia_, which is the sum-of-squares distance from each point of a cluster to its centroid.\n","\n","This inertia is thought of as a measure of the compactness of the cluster, or as 'a measure of how internally coherent cluster are'.\n","\n","We can view some metrics of the clustering:"],"metadata":{"id":"-Kx3IT8vkAPK"}},{"cell_type":"code","source":["from sklearn import metrics"],"metadata":{"id":"cKa2LoAHplfw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_cluster(n_clusters, init, max_iter, n_init):\n","  clf = cluster.KMeans(n_clusters=n_clusters, init=init, random_state=0, max_iter=max_iter, n_init=n_init)\n","  clf.fit(X)\n","  print('Final evaluation of the clustering:')\n","  print(f'Inertia: {clf.inertia_:.2f}')\n","  print(f'Adjusted_rand_score: {metrics.adjusted_rand_score(y.ravel(), clf.labels_):.2f}')\n","  print(f'Homogeneity: {metrics.homogeneity_score(y.ravel(), clf.labels_):.2f}')\n","  print(f'Completeness: {metrics.completeness_score(y.ravel(), clf.labels_):.2f}')\n","  print(f'V_measure: {metrics.v_measure_score(y.ravel(), clf.labels_):.2f}')\n","  print(f'Silhouette: {metrics.silhouette_score(X, clf.labels_, metric=\"euclidean\"):.2f}')\n","  plt.title(f'K-means, {n_clusters} clusters, {max_iter} iterations, {n_init} n_init, {init}', size=14)\n","  plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\n","  plt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\n","  plt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n","  x = np.linspace(-5,15, 200)\n","  XX, YY = np.meshgrid(x, x)\n","  data=np.c_[XX.ravel(),YY.ravel()]\n","  Z=clf.predict(data)\n","  plt.imshow(Z.reshape(sz), cmap='hsv', interpolation='bilinear', origin='lower', extent=(-5, 15, -5, 15), alpha=0.6, vmin=0, vmax=n_clusters-1)\n","  fig = plt.gcf()\n","  fig.set_size_inches((6, 5))\n","  plt.show()\n","\n","test_cluster(3, 'k-means++', 300, 10)"],"metadata":{"id":"1V_tMzi0MvyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_cluster(3, 'random', 2, 2)"],"metadata":{"id":"Us5UaewBoX3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_cluster(3, 'k-means++', 1, 1)"],"metadata":{"id":"vley0Zpwqn3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_cluster(8, 'k-means++', 300, 10)"],"metadata":{"id":"QU6S0Q3IsKh9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here are some issues with k-means:\n","\n","* Inertia makes the assumption that the clusters are convex and isotropic, which is not always the case.  It responds poorly to elongated clusters, or manifolds with irregular shapes.\n","\n","* Given enough time, K-means will always converge, although it may be converging on a local minima.\n","\n","* The algorithm requires the number of clusters to be specified.\n","\n","* It scales well to a large number of samples and has been used across a large range of application areas in many different fields.\n","\n","* The computation ios often done several times (with an n_init > 1), with different initializations of the centroids.  This is to help alleviate the local minima issue.\n","\n","* The _k-means++_ initialization scheme initializes the centroids to be generally distant from each other, leading to better results than the _random_ initialization scheme.\n"],"metadata":{"id":"uSbO_qkxzvBY"}},{"cell_type":"markdown","source":["## Spectral Clustering\n","\n","The k-means clustering groups the data to optimize \"compactness\".\n","\n","Spectral Clustering groups the data to optimize \"connectivity\".\n","\n","For example, Spectral Clustering might compute a _similarity matrix_ giving a measure of the similarity between any two samples in the dataset.  The algorithm then merges or clusters the samples that are the closest together.\n","\n","The expectation is that two points in the same cluster are close together.  However, two distant points might be in the same cluster if there is a 'path' between them (of other samples), such that the distance between subsequent pairs of samples on the path are small.\n","\n","Scikit-learn has some tools in its library to generate datasets for use in illustrating these algorithms:"],"metadata":{"id":"srYBnHG01u3P"}},{"cell_type":"code","source":["from sklearn.datasets import make_blobs, make_moons\n","\n","X, labels_true = make_blobs(n_samples = 1000, centers = 3, cluster_std=[1.7,1.7,1.7])\n","[Xmoons, ymoons] = make_moons(n_samples=300, noise=0.05)\n","\n","plt.subplot(1,2,1)\n","plt.scatter(X[:, 0], X[:, 1], c='r', marker='o', s=20)\n","plt.axis('equal')\n","plt.title(\"Compact Clusters\", size=14)\n","\n","plt.subplot(1,2,2)\n","plt.scatter(Xmoons[:,0], Xmoons[:,1], c='r', marker='o', s=20)\n","plt.axis('equal')\n","plt.title(\"Connectivity-Based Clusters\", size=14)\n","fig = plt.gcf()\n","fig.set_size_inches((11,6))"],"metadata":{"id":"VFsN4DiRshHh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can apply the Spectral clustering on the two moons problem:"],"metadata":{"id":"ndMzXI755EL5"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import kneighbors_graph\n","from sklearn.metrics import euclidean_distances\n","colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n","colors = np.hstack([colors] * 20)\n","\n","# compute the distances\n","distances = euclidean_distances(Xmoons)\n","\n","spectral = cluster.SpectralClustering(n_clusters=2, affinity=\"nearest_neighbors\")\n","spectral.fit(Xmoons)\n","y_pred = spectral.labels_.astype(int)"],"metadata":{"id":"6Nv0Fq0E4NiQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now visualize the results:"],"metadata":{"id":"WNRyup506c5l"}},{"cell_type":"code","source":["plt.subplot(1,2,1)\n","plt.scatter(Xmoons[:, 0], Xmoons[:, 1], c='r', marker='o', s=20)\n","plt.axis('equal')\n","\n","plt.subplot(1,2,2)\n","plt.scatter(Xmoons[y_pred==0, 0], Xmoons[y_pred==0, 1], c='b', marker='o', s=20)\n","plt.scatter(Xmoons[y_pred==1, 0], Xmoons[y_pred==1, 1], c='y', marker='o', s=20)\n","plt.axis('equal')\n","\n","fig = plt.gcf()\n","fig.set_size_inches((11, 6))\n","plt.show()"],"metadata":{"id":"MJAZZbiJ6Dff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the book, this showed a perfect clustering of the points.  In our example here, one of the horns of a moon is in the wrong cluster.  However, next time we run this, it might be correct!\n","\n","We can compare the results to K-means:"],"metadata":{"id":"Ere-3voI8P7q"}},{"cell_type":"code","source":["plt.subplot(1,2,1)\n","plt.scatter(Xmoons[:, 0], Xmoons[:, 1], c='r', marker='o', s=20)\n","plt.axis('equal')\n","\n","clf = cluster.KMeans(n_clusters=2, init='k-means++')\n","clf.fit(Xmoons)\n","y_pred = clf.predict(Xmoons)\n","\n","plt.subplot(1,2,2)\n","plt.scatter(Xmoons[y_pred==0, 0], Xmoons[y_pred==0, 1], c='b', marker='o', s=20)\n","plt.scatter(Xmoons[y_pred==1, 0], Xmoons[y_pred==1, 1], c='y', marker='o', s=20)\n","plt.axis('equal')\n","\n","fig = plt.gcf()\n","fig.set_size_inches((11, 6))\n","plt.show()"],"metadata":{"id":"v1q-a5cW7Qpg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, the K-means looks for spherical clusters, so it has a different result than the connection-based Spectral clusterer."],"metadata":{"id":"E7w-z2KR9K4w"}},{"cell_type":"code","source":[],"metadata":{"id":"6Zab_i0m8s9W"},"execution_count":null,"outputs":[]}]}